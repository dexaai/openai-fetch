import type { CompletionResponseUsage } from './completion';
import { z } from 'zod';

/** Chat completion model name */
const ChatCompletionModel = z.union([
  z.literal('gpt-3.5-turbo'),
  z.literal('gpt-3.5-turbo-0301'),
  z.string(),
]);

/** Chat message role */
const ChatMessageRoleSchema = z.union([
  z.literal('system'),
  z.literal('user'),
  z.literal('assistant'),
]);
export type ChatMessageRole = z.infer<typeof ChatMessageRoleSchema>;

/** Chat completion request message */
const ChatCompletionRequestMessage = z.object({
  /** The role of the author of this message. */
  role: ChatMessageRoleSchema,
  /** The contents of the message */
  content: z.string(),
  /** The name of the user in a multi-user chat */
  name: z.string().nullish(),
});

export const ChatCompletionParamsSchema = z.object({
  /** ID of the model to use. Currently, only `gpt-3.5-turbo` and `gpt-3.5-turbo-0301` are supported. */
  model: ChatCompletionModel,
  /** The messages to generate chat completions for, in the [chat format](/docs/guides/chat/introduction). */
  messages: z.array(ChatCompletionRequestMessage),
  /** What sampling temperature to use, between 0 and 2. Higher values like 0.8 will make the output more random, while lower values like 0.2 will make it more focused and deterministic. We generally recommend altering this or `top_p` but not both. */
  temperature: z.number().nullish(),
  /** An alternative to sampling with temperature, called nucleus sampling, where the model considers the results of the tokens with top_p probability mass. So 0.1 means only the tokens comprising the top 10% probability mass are considered. We generally recommend altering this or `temperature` but not both. */
  top_p: z.number().nullish(),
  /** Up to 4 sequences where the API will stop generating further tokens. The returned text will not contain the stop sequence. */
  stop: z.union([z.string(), z.array(z.string())]).nullish(),
  /** The maximum number of tokens allowed for the generated answer. By default, the number of tokens the model can return will be (4096 - prompt tokens). */
  max_tokens: z.number().nullish(),
  /** Number between -2.0 and 2.0. Positive values penalize new tokens based on whether they appear in the text so far, increasing the model's likelihood to talk about new topics. [See more information about frequency and presence penalties.](/docs/api-reference/parameter-details) */
  presence_penalty: z.number().nullish(),
  /** Number between -2.0 and 2.0. Positive values penalize new tokens based on their existing frequency in the text so far, decreasing the model's likelihood to repeat the same line verbatim. [See more information about frequency and presence penalties.](/docs/api-reference/parameter-details) */
  frequency_penalty: z.number().nullish(),
  /** Modify the likelihood of specified tokens appearing in the completion. Accepts a json object that maps tokens (specified by their token ID in the tokenizer) to an associated bias value from -100 to 100. Mathematically, the bias is added to the logits generated by the model prior to sampling. The exact effect will vary per model, but values between -1 and 1 should decrease or increase likelihood of selection; values like -100 or 100 should result in a ban or exclusive selection of the relevant token. */
  logit_bias: z.record(z.number()).nullish(),
  /** A unique identifier representing your end-user, which can help OpenAI to monitor and detect abuse. [Learn more](/docs/guides/safety-best-practices/end-user-ids). */
  user: z.string().nullish(),
  /**
   * NOT SUPPORTED
   * - n
   * - stream
   */
});

export type ChatCompletionParams = z.input<typeof ChatCompletionParamsSchema>;

export type ChatCompletionResponse = {
  id: string;
  object: string;
  created: number;
  model: string;
  choices: ChatCompletionResponseChoices;
  usage?: CompletionResponseUsage;
};

export type ChatResponseMessage = {
  /** The role of the author of this message. */
  role: ChatMessageRole;
  /** The contents of the message */
  content: string;
};

export type ChatCompletionResponseChoices = {
  index?: number;
  finish_reason?: string;
  message?: ChatResponseMessage;
  /** Used instead of `message` when streaming */
  delta?: ChatResponseMessage;
}[];
